## 1. 개요: 웹에서의 실시간 스트리밍

웹 브라우저는 RTSP/RTP와 같은 스트리밍 프로토콜을 직접 지원하지 않습니다. 따라서 IP 카메라 등에서 전송되는 RTSP 스트림을 웹에서 재생하려면, 중간에서 스트림 포맷을 변환하고 웹 브라우저가 이해할 수 있는 형식으로 공급해주는 **서버 측 기술**과 이 데이터를 받아 재생하는 **클라이언트 측 기술**이 모두 필요합니다.

이 가이드는 클라이언트 측 기술인 **Media Source Extensions(MSE)**와 서버 측 기술인 **트랜스먹싱(Transmuxing)**을 중심으로 전체적인 구현 방법을 다룹니다.

## 2. 클라이언트 측 기술: Media Source Extensions (MSE)

Media Source Extensions(MSE)는 자바스크립트를 통해 `<video>`나 `<audio>` 태그로 미디어 데이터를 직접 공급할 수 있게 해주는 API입니다. 이를 통해 동적인 미디어 스트리밍을 구현할 수 있습니다.

### 2.1 핵심 개념: MediaSource와 SourceBuffer

- **`MediaSource`**: 스트리밍 세션 전체를 관리하는 객체입니다. `<video>` 태그에 미디어 소스로 자신을 등록하고, 실제 데이터를 처리할 `SourceBuffer` 객체를 관리합니다.
    
- **`SourceBuffer`**: 실제 미디어 데이터를 받아들이고 처리하는 객체입니다. `appendBuffer()` 메서드를 통해 원시 바이너리 데이터를 수용하고, 파싱 및 디코딩을 거쳐 버퍼에 저장합니다.
    

### 2.2 동작 원리 및 데이터 흐름

1. **객체 생성 및 연결**: `new MediaSource()`로 객체를 만들고, `URL.createObjectURL()`을 통해 `<video>` 태그의 `src`에 할당합니다.
    
2. **`sourceopen` 이벤트**: 미디어 요소와 `MediaSource`가 성공적으로 연결되면 `sourceopen` 이벤트가 발생합니다. 이 시점에서 `SourceBuffer`를 생성하고, `MediaSource.isTypeSupported()`로 미디어 포맷 지원 여부를 확인합니다.
    
3. **데이터 파이프라인**: WebSocket 등을 통해 서버로부터 받은 이진(binary) 데이터를 `SourceBuffer`에 연속적으로 추가합니다. `SourceBuffer`는 이 데이터를 파싱하고 디코딩하여 `<video>` 태그가 재생할 수 있는 상태로 만듭니다.
    

### 2.3 데이터 관리 및 버퍼링

`SourceBuffer`는 한 번에 하나의 작업만 수행할 수 있으므로, 연속적으로 데이터를 추가하려면 작업이 끝나는 시점을 정확히 파악하는 이벤트 기반 로직이 필요합니다.

- **`updating` 속성**: `SourceBuffer`가 현재 데이터 추가 또는 삭제 작업 중인지를 나타냅니다. 새로운 작업을 시작하기 전에 이 속성값이 `false`인지 반드시 확인해야 합니다.
    
- **`updateend` 이벤트**: `appendBuffer()` 또는 `remove()` 작업이 완료되면 발생하는 이벤트입니다. 이 이벤트를 통해 작업 완료 시점을 파악하고 다음 데이터를 추가하는 이벤트 기반 로직을 구현하는 것이 효율적입니다.
    
- **데이터 큐**: 서버에서 데이터를 받는 즉시 버퍼에 추가하는 대신, 큐에 데이터를 쌓아두고 `updateend` 이벤트가 발생할 때마다 순차적으로 꺼내 처리합니다. 큐가 비어있는 경우에는 서버에 다음 데이터를 요청하고 버퍼링 UI를 표시하는 로직을 추가할 수 있습니다.
    

### 2.4 클라이언트 측 구현 예시 (HTML/CSS/JS)

```html
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RTSP 실시간 스트리밍 클라이언트</title>
    <script src="[https://cdn.tailwindcss.com](https://cdn.tailwindcss.com)"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            color: #1f2937;
        }
        .video-container {
            @apply bg-white p-6 rounded-2xl shadow-xl max-w-4xl w-full;
        }
        video {
            @apply w-full rounded-xl border border-gray-200;
        }
        #status {
            @apply mt-4 p-3 rounded-xl text-center text-sm font-medium;
        }
    </style>
</head>
<body class="bg-gray-100">

<div class="video-container">
    <h1 class="text-3xl font-bold mb-4 text-center">RTSP 실시간 스트리밍</h1>
    <video id="videoPlayer" controls muted autoplay></video>
    <p id="status" class="bg-blue-100 text-blue-700">연결을 기다리는 중...</p>
</div>

<script>
    document.addEventListener('DOMContentLoaded', () => {
        const videoElement = document.getElementById('videoPlayer');
        const statusElement = document.getElementById('status');
        const websocketUrl = 'ws://localhost:8080'; // 서버의 WebSocket 주소

        let mediaSource = null;
        let sourceBuffer = null;
        let videoQueue = []; // 미디어 세그먼트를 저장할 큐
        let ws = null;

        // WebSocket 연결 및 이벤트 핸들러 설정
        function setupWebSocket() {
            if (ws) {
                ws.close();
            }
            ws = new WebSocket(websocketUrl);

            ws.onopen = () => {
                console.log('WebSocket 연결이 성공적으로 열렸습니다.');
                statusElement.textContent = '서버에 연결되었습니다.';
            };

            ws.onmessage = (event) => {
                const data = event.data;
                // ArrayBuffer 형식의 이진 데이터를 큐에 추가
                if (data instanceof ArrayBuffer) {
                    videoQueue.push(data);
                    // 버퍼가 준비되면 큐 처리 시작
                    processQueue();
                } else {
                    console.log('텍스트 메시지 수신:', data);
                }
            };

            ws.onclose = () => {
                console.log('WebSocket 연결이 종료되었습니다.');
                statusElement.textContent = '서버 연결이 종료되었습니다.';
            };

            ws.onerror = (error) => {
                console.error('WebSocket 오류 발생:', error);
                statusElement.textContent = 'WebSocket 오류 발생. 콘솔을 확인해주세요.';
            };
        }

        // MediaSource 객체 설정 및 이벤트 핸들러
        function setupMediaSource() {
            mediaSource = new MediaSource();
            videoElement.src = URL.createObjectURL(mediaSource);

            mediaSource.addEventListener('sourceopen', () => {
                console.log('MediaSource 연결이 열렸습니다.');
                const mime = 'video/mp4; codecs="avc1.42E01E"'; // 코덱은 FFmpeg 출력에 맞춰야 함

                if (MediaSource.isTypeSupported(mime)) {
                    sourceBuffer = mediaSource.addSourceBuffer(mime);

                    // 핵심: updateend 이벤트 리스너를 통해 버퍼링 작업 관리
                    sourceBuffer.addEventListener('updateend', () => {
                        // 이전 버퍼링 작업이 끝나면 큐 처리 계속
                        processQueue();
                    });

                    statusElement.textContent = '스트리밍 준비 완료. 데이터 수신 중...';
                    processQueue(); // 초기 큐 처리 시작
                } else {
                    statusElement.textContent = `브라우저가 MIME 타입(${mime})을 지원하지 않습니다.`;
                    console.error('MIME 타입 지원 안됨:', mime);
                }
            });
        }

        // 큐에 있는 데이터를 순차적으로 SourceBuffer에 추가
        function processQueue() {
            // SourceBuffer가 존재하고, 작업 중이 아니며, 큐에 데이터가 있을 때
            if (sourceBuffer && !sourceBuffer.updating && videoQueue.length > 0) {
                const bufferData = videoQueue.shift();
                try {
                    sourceBuffer.appendBuffer(bufferData);
                } catch (e) {
                    console.error('버퍼에 데이터를 추가하는 중 오류 발생:', e);
                    // 오류 발생 시 큐 비우고 재시작 로직 고려
                }
            }
        }

        // 애플리케이션 시작
        setupWebSocket();
        setupMediaSource();
    });
</script>
</body>
</html>
```

## 3. 서버 측 기술: RTSP 트랜스먹싱 서버

RTSP 스트림을 웹에서 재생하기 위해서는 서버에서 스트림을 받아 웹 친화적인 포맷으로 변환해야 합니다. 이 과정은 코덱을 변경하지 않는 **트랜스먹싱** 방식으로 구현하여 서버 부하를 최소화합니다.

### 3.1 트랜스코딩 vs. 트랜스먹싱

- **트랜스코딩**: 압축된 비디오/오디오를 **디코딩 후 다시 인코딩**하는 과정입니다. 코덱, 해상도, 비트레이트를 변경할 때 사용되며, CPU/GPU 자원을 많이 소모합니다.
    
- **트랜스먹싱**: 코덱 변경 없이 **컨테이너 포맷만 변경**하는 과정입니다. RTSP 스트림을 fMP4와 같은 웹 포맷으로 재구성하며, 서버 부하가 훨씬 적습니다.
    

### 3.2 FFmpeg와 WebSocket을 활용한 아키텍처

트랜스먹싱 서버는 FFmpeg를 통해 RTSP 스트림을 fMP4로 변환하고, WebSocket을 통해 클라이언트에 실시간으로 전송합니다.

1. **FFmpeg**: RTSP 스트림을 입력받아 `librtmp` 라이브러리를 통해 fMP4 데이터로 변환합니다. `-c copy` 옵션으로 코덱 복사만 수행하며, `-movflags frag_keyframe+empty_moov+default_base_moof` 옵션을 사용해 fMP4 포맷을 만듭니다.
    
2. **WebSocket 서버**: FFmpeg의 출력 데이터를 파이프(pipe)로 받아, 연결된 모든 클라이언트에게 실시간으로 브로드캐스트합니다.
    
3. **클라이언트**: WebSocket을 통해 바이너리 데이터를 수신하여 MSE의 `SourceBuffer`에 공급합니다.
    

### 3.3 서버 측 구현 예시 (FFmpeg & Node.js)

```javascript
const WebSocket = require('ws');
const { spawn } = require('child_process');

const wss = new WebSocket.Server({ port: 8080 });

wss.on('connection', ws => {
    console.log('클라이언트가 연결되었습니다.');
    
    // FFmpeg 프로세스 시작
    const ffmpeg = spawn('ffmpeg', [
        '-rtsp_transport', 'tcp', // TCP 전송을 통해 안정적인 스트림 확보
        '-i', 'rtsp://your_rtsp_stream_url', // 입력 RTSP 스트림 URL
        '-c', 'copy', // 코덱 변경 없이 복사 (트랜스먹싱의 핵심)
        '-an', // 오디오 스트림을 무시 (비디오만 필요한 경우)
        '-vcodec', 'copy', // 비디오 코덱 복사
        '-bsf:v', 'h264_mp4toannexb', // H.264 비트스트림 필터 적용 (호환성 목적)
        '-movflags', 'frag_keyframe+empty_moof+default_base_moof', // fMP4 포맷을 위한 핵심 옵션
        '-f', 'mp4', // 출력 컨테이너 포맷을 MP4로 지정
        'pipe:1' // 표준 출력(stdout)으로 바이너리 데이터 전송
    ]);

    // FFmpeg의 stdout을 WebSocket으로 전달
    ffmpeg.stdout.on('data', data => {
        ws.send(data);
    });

    // 연결 종료 시 FFmpeg 프로세스 종료
    ws.on('close', () => {
        console.log('클라이언트 연결이 종료되었습니다. FFmpeg 프로세스를 종료합니다.');
        ffmpeg.kill('SIGINT');
    });

    ffmpeg.stderr.on('data', data => {
        console.error(`FFmpeg stderr: ${data}`);
    });
});

console.log('WebSocket 서버가 8080 포트에서 실행 중입니다.');

```

## 4. 최종 결론

웹 기반 실시간 스트리밍은 단순히 `<video>` 태그를 사용하는 것을 넘어, 다음과 같은 기술 스택을 결합하여 구현됩니다.

1. **서버 측**: RTSP와 같은 원시 스트림을 수신하고, FFmpeg를 이용한 트랜스먹싱을 통해 웹 친화적인 fMP4 포맷으로 실시간 변환합니다.
    
2. **전송 채널**: WebSocket과 같이 저지연 푸시 기반의 통신 채널을 사용하여 변환된 데이터를 클라이언트에 끊김 없이 전송합니다.
    
3. **클라이언트 측**: `MediaSource Extensions` API를 사용하여 서버로부터 받은 데이터를 직접 `<video>` 태그의 버퍼에 공급하고 재생합니다.
    

이러한 구조는 서버의 부하를 최소화하면서 여러 클라이언트에게 안정적이고 빠른 실시간 스트리밍 경험을 제공할 수 있습니다.